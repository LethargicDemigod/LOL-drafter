{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnL5rfrlt56ni/6VNkSPMF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LethargicDemigod/LOL-drafter/blob/main/LOL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGsXRVR2JtkW",
        "outputId": "4ffbd357-7eb5-4d82-e94f-39dfba4314f2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1EZyO05zA_Z9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import torch\n",
        "\n",
        "def load_champion_vectors(csv_path):\n",
        "    champ_to_vec = {}\n",
        "    with open(csv_path, newline=\"\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            name = row[\"champion\"]\n",
        "            vec = torch.tensor([\n",
        "                float(row[\"damage\"]),\n",
        "                float(row[\"toughness\"]),\n",
        "                float(row[\"control\"]),\n",
        "                float(row[\"mobility\"]),\n",
        "                float(row[\"utility\"])\n",
        "            ], dtype=torch.float32)\n",
        "            champ_to_vec[name] = vec\n",
        "    return champ_to_vec\n",
        "\n"
      ],
      "metadata": {
        "id": "kyhizoBICYP-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def match_loader(jsonl_path,start=0,end=None):\n",
        "    with open(jsonl_path, 'r') as f:\n",
        "        for idx, line in enumerate(f):\n",
        "            if idx < start:\n",
        "                continue\n",
        "            if end is not None and idx >= end:\n",
        "                break\n",
        "            try:\n",
        "                match = json.loads(line)\n",
        "                yield idx, match\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Skipping line-{idx}\")\n",
        "                continue\n",
        "\n"
      ],
      "metadata": {
        "id": "x5IGLZtRCdT9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChampModel(nn.Module):\n",
        "    def __init__(self, champion_vectors, learnable=False):\n",
        "        super().__init__()\n",
        "        self.champ_names = list(champion_vectors.keys())\n",
        "        initial_vecs = torch.stack([champion_vectors[name] for name in self.champ_names])\n",
        "        self.embedding = nn.Embedding(171,25)\n",
        "        nn.init.xavier_uniform_(self.embedding.weight)\n",
        "        self.name_to_idx = {name: i for i, name in enumerate(self.champ_names)}\n",
        "\n",
        "        self.attn1 = nn.MultiheadAttention(embed_dim=25, num_heads=5, batch_first=True)\n",
        "        self.norm1=nn.LayerNorm(25)\n",
        "        self.mlp1= nn.Sequential(\n",
        "            nn.Linear(25, 25),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(25, 25)\n",
        "        )\n",
        "        self.norm2=nn.LayerNorm(25)\n",
        "        self.attn2 = nn.MultiheadAttention(embed_dim=25, num_heads=5, batch_first=True)\n",
        "        self.norm3=nn.LayerNorm(25)\n",
        "        self.mlp2= nn.Sequential(\n",
        "            nn.Linear(25, 25),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(25, 25)\n",
        "        )\n",
        "        self.norm4=nn.LayerNorm(25)\n",
        "\n",
        "        self.output = nn.Linear(25, 1)\n",
        "\n",
        "    def freeze_embeddings(self):\n",
        "        self.embedding.weight.requires_grad = False\n",
        "\n",
        "    def unfreeze_embeddings(self):\n",
        "        self.embedding.weight.requires_grad = True\n",
        "\n",
        "    def forward(self, batch_teams):\n",
        "      batch_idxs = [\n",
        "        [self.name_to_idx[name] for name in team]\n",
        "        for team in batch_teams\n",
        "      ]\n",
        "\n",
        "      idx_tensor = torch.tensor(batch_idxs, device=self.embedding.weight.device)\n",
        "      vectors = self.embedding(idx_tensor)\n",
        "\n",
        "      attn_out1, _ = self.attn1(vectors, vectors, vectors)\n",
        "      x=self.mlp1(attn_out1)\n",
        "      attn_out2, _ = self.attn2(x, x, x)\n",
        "      x=self.mlp2(attn_out2)\n",
        "      pooled = attn_out1.mean(dim=1)\n",
        "      return self.output(pooled).squeeze(-1)\n",
        "    def freeze_attention(self):\n",
        "      for param in self.attn1.parameters():\n",
        "        param.requires_grad = False\n",
        "      for param in self.attn2.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    def unfreeze_attention(self):\n",
        "      for param in self.attn1.parameters():\n",
        "        param.requires_grad = True\n",
        "      for param in self.attn2.parameters():\n",
        "        param.requires_grad = True\n"
      ],
      "metadata": {
        "id": "n-WtD5oOCnqN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_phase(model, start, end, jsonl_path, optimizer, batch_size=32):\n",
        "    batch_blue, batch_red, batch_labels = [], [], []\n",
        "\n",
        "    total = end - start\n",
        "    iterator = tqdm(match_loader(jsonl_path, start, end), total=total, desc=\"Training\")\n",
        "\n",
        "    for idx, match in iterator:\n",
        "        blue = [p[\"championName\"] for p in match[\"blueTeam\"]]\n",
        "        red = [p[\"championName\"] for p in match[\"redTeam\"]]\n",
        "        winner = match[\"winningTeam\"]\n",
        "\n",
        "        batch_blue.append(blue)\n",
        "        batch_red.append(red)\n",
        "        batch_labels.append(0 if winner == 100 else 1)\n",
        "\n",
        "        if len(batch_blue) == batch_size:\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            blue_scores = model(batch_blue)\n",
        "            red_scores = model(batch_red)\n",
        "\n",
        "            logits = torch.sigmoid(blue_scores-red_scores)\n",
        "            labels = torch.tensor(batch_labels, dtype=torch.float32, device=logits.device)\n",
        "\n",
        "            loss = F.binary_cross_entropy_with_logits(logits,labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            iterator.set_postfix(loss=loss.item())\n",
        "\n",
        "            batch_blue, batch_red, batch_labels = [], [], []\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HuFwS9_NCr1G"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GEIRodZCv_U",
        "outputId": "559de8b5-75ed-41f8-9bfa-3a795f1813dd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "jsonl=jsonl_path = \"/content/drive/My Drive/deduplicated_matches.jsonl\"\n",
        "vector_embeddings=\"/content/drive/My Drive/champion_playstyle_vectors.csv\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlQlQpceeV4t",
        "outputId": "9f179ffe-3781-43da-e5b4-99474a13c2ad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch , random\n",
        "champion_vectors = load_champion_vectors(vector_embeddings)\n",
        "champion_names = list(champion_vectors.keys())\n",
        "new_champ_vectors={}\n",
        "for champ in champion_names :\n",
        "   new_champ_vectors[champ]=torch.rand(25)\n",
        "cleaned_champ_vectors = {\n",
        "    k: v for k, v in champion_vectors.items()\n",
        "    if k is not None and k != \"None\"\n",
        "}\n",
        "print(len(cleaned_champ_vectors))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VkpI_75wAqW",
        "outputId": "12b7efa1-f176-4b91-e193-d14d032f8f3c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model = ChampModel(cleaned_champ_vectors, learnable=True).to(device)\n",
        "for epochs in range(30):\n",
        "  print(f\"epoch{epochs}\")\n",
        "  model.freeze_attention()\n",
        "  optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-2)\n",
        "\n",
        "  train_phase(model, 0, 3_570_000, jsonl, optimizer,batch_size=32)\n",
        "\n",
        "for n_epochs in range(60):\n",
        "  print(f\"epoch{n_epochs+5}\")\n",
        "  model.unfreeze_attention()\n",
        "  optimizer=torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
        "  train_phase(model, 0, 3_570_000, jsonl, optimizer, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4KbqseLDczc",
        "outputId": "5a679dbb-93dd-4c96-dfa9-12556d45c682"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:02<00:00, 2740.68it/s, loss=0.685]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:02<00:00, 2733.63it/s, loss=0.67]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:02<00:00, 2720.15it/s, loss=0.662]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:03<00:00, 2698.10it/s, loss=0.658]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:01<00:00, 2769.61it/s, loss=0.658]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:02<00:00, 2756.23it/s, loss=0.658]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:09<00:00, 2477.49it/s, loss=0.667]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:02<00:00, 2732.16it/s, loss=0.681]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:03<00:00, 2699.80it/s, loss=0.691]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:01<00:00, 2768.90it/s, loss=0.699]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:05<00:00, 2607.51it/s, loss=0.689]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:07<00:00, 2528.28it/s, loss=0.694]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:05<00:00, 2602.16it/s, loss=0.708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:08<00:00, 2513.24it/s, loss=0.701]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:06<00:00, 2565.81it/s, loss=0.704]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:06<00:00, 2589.28it/s, loss=0.724]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:10<00:00, 2418.27it/s, loss=0.713]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:05<00:00, 2601.71it/s, loss=0.696]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:07<00:00, 2521.18it/s, loss=0.72]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:05<00:00, 2602.94it/s, loss=0.726]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:06<00:00, 2558.16it/s, loss=0.736]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:08<00:00, 2510.88it/s, loss=0.713]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:06<00:00, 2559.71it/s, loss=0.717]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:07<00:00, 2545.81it/s, loss=0.716]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:08<00:00, 2514.48it/s, loss=0.703]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:08<00:00, 2510.69it/s, loss=0.726]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:07<00:00, 2531.97it/s, loss=0.702]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:06<00:00, 2555.84it/s, loss=0.722]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:09<00:00, 2459.92it/s, loss=0.704]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 171000/171000 [01:07<00:00, 2516.80it/s, loss=0.704]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = model.embedding.weight.data.cpu().numpy()\n",
        "idx_to_name = {i: name for name, i in model.name_to_idx.items()}\n",
        "\n",
        "for i, vector in enumerate(embeddings):\n",
        "    name = idx_to_name[i]\n",
        "    print(f\"{name}: {vector}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBqRVuOpCavP",
        "outputId": "a5a95354-73ba-44e6-9b74-88bd31448627"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Annie: [-0.9226638   2.2328565   1.4044354  -0.9282027   1.8227818   2.4394674\n",
            " -2.7681637  -0.37173468  3.8683543  -2.0054867   1.0954802  -0.23527133\n",
            "  2.1538973  -2.1296608  -0.84010077  2.896468   -3.766542    1.2972755\n",
            "  1.0595247  -1.7364905  -1.8591913  -0.15433612  2.193149   -1.7030712\n",
            " -2.3658614 ]\n",
            "Olaf: [ 1.7292154  -2.2932682   0.5177614  -1.0394078  -3.9217901  -2.6070697\n",
            "  3.587276    0.47967356  1.0045431  -0.3913138  -0.04576604  0.55376995\n",
            "  0.00787961  1.608246   -1.3148997  -0.7754894   1.8027974  -0.6086022\n",
            " -5.849873    1.1034861   1.2468374   1.6073565   3.3041658  -2.5351908\n",
            "  4.491555  ]\n",
            "Galio: [ 3.409223   -3.4448993   0.7520576  -1.9100348  -0.62884027  3.0229049\n",
            "  1.2549347   2.915588   -2.7925508  -1.1386472   3.0739322  -0.11655661\n",
            " -1.7208654  -0.7874046   0.3393953  -0.15283002  3.8377323  -0.3996694\n",
            " -0.9601866   2.4289198  -1.917128   -1.0113384  -0.03924594  2.8437512\n",
            "  2.0903134 ]\n",
            "TwistedFate: [-0.7660138  -0.3668051  -2.3430972   1.519925   -2.0997856   0.5673228\n",
            "  1.2751259  -2.335652   -0.63660413  1.0768836  -1.3170633  -1.0459027\n",
            " -2.4533753   0.5567674  -0.11907768 -2.9785788  -0.8966265   1.4506738\n",
            "  0.24225663 -0.29398978 -0.34650925  0.24393557 -0.36191127 -1.5639721\n",
            " -1.9521252 ]\n",
            "XinZhao: [ 0.22099762 -0.2148561  -2.2688835   0.35626763 -1.650426    2.620602\n",
            " -0.32808393 -1.0947933  -1.5612764   2.700154   -0.22825086 -0.20335668\n",
            "  1.9716353  -1.370793    0.07924335 -1.8472228   1.0044113  -1.8802481\n",
            " -1.7948002   1.2026564  -0.2234619   1.0223243  -1.8295798  -1.5060148\n",
            " -2.790633  ]\n",
            "Urgot: [ 0.65486264 -0.798964   -0.7701493  -0.01665577 -3.015672   -0.5197941\n",
            "  1.1872213  -0.05277336  1.308229   -0.15181641  0.31239486  0.40384617\n",
            " -1.6596208   0.90771806  1.624607    0.8357202   0.4661607   0.41239882\n",
            " -1.8154734  -0.22056007  0.5846841  -1.2723583   0.75968295 -0.7866235\n",
            "  1.2981405 ]\n",
            "Leblanc: [-1.4522574   0.42719826  0.63110626 -1.9165518   2.9096906  -3.3988678\n",
            "  0.44511396  1.1321194  -0.16955185  0.505322    1.0781592   0.9689768\n",
            " -2.3857777   1.54896    -0.01240984 -0.17936832 -1.9014727   2.6072295\n",
            "  1.0074319  -0.46592745  1.9976957   0.19964421 -1.3021681   2.0355318\n",
            " -1.5637488 ]\n",
            "Vladimir: [-0.26086926  1.821567   -0.39162746 -2.4223516   1.0191679  -0.8864115\n",
            "  1.8896524   1.0336109  -2.1527178   1.8714368   0.72162515  1.0395097\n",
            " -0.21672441 -0.01707234  1.4553599  -0.63390243  0.5955066  -2.0847325\n",
            " -0.8628738  -1.0662379  -0.03228622 -0.7650894  -0.00936178 -2.4007843\n",
            "  1.0055761 ]\n",
            "FiddleSticks: [ 1.1989775  -2.727794   -1.5838423   0.37332276 -1.2384181   0.49338293\n",
            "  1.6546385  -0.86582905 -1.5223056  -3.653811    1.0282005   2.379945\n",
            "  1.6960343   0.0261174  -0.8267665  -2.9822342   1.4128134  -0.2813329\n",
            " -2.0936775   1.9922875  -0.820244    1.0052012  -2.4768488  -0.61887085\n",
            "  0.38229626]\n",
            "Kayle: [ 1.7909976   1.1182203  -0.672404    1.0151612  -0.88349783 -3.7003644\n",
            "  0.84501624  0.10996949 -1.9195899   0.7383838  -0.23092644 -0.5807744\n",
            "  0.8443079  -0.39731377  2.9589157   2.8660097  -1.6135246   0.4741504\n",
            " -0.9223782  -1.644607   -1.3732373  -1.4832492   0.15912646 -1.373821\n",
            "  1.0503365 ]\n",
            "MasterYi: [ 0.661851    2.07317     1.069008   -2.2776103   2.5893693  -0.11742625\n",
            " -0.78683096  1.9284494   0.619787   -1.6555481   1.2928898  -0.8203205\n",
            " -1.28227    -1.860573   -0.57465714 -0.49839193  0.4473727  -0.352587\n",
            " -2.408983   -1.3108749  -0.0530199   0.11110848  2.75114    -0.7158591\n",
            "  1.1862128 ]\n",
            "Alistar: [-0.4684013  -1.8472744   2.4534638   0.3891478  -1.5755397  -0.20759825\n",
            "  1.072527    0.14277606 -1.3212726   0.09974192  0.46628788  1.1326147\n",
            "  1.9120288  -0.4346411  -1.0127853  -0.95370466 -0.03272781  0.36445382\n",
            " -0.5739878   2.467608    0.2001484  -0.05866345 -0.30143228  1.989615\n",
            "  1.0873687 ]\n",
            "Ryze: [-0.65809786  0.35190824  0.79866993 -0.2570978   1.5171112  -2.2908509\n",
            " -0.8368575   3.4952588  -1.1232834   1.0478485   0.11896829  0.7990932\n",
            "  1.4635173  -2.2856057  -1.0562487   0.83118325  0.49992922 -1.4960291\n",
            "  0.3098452  -2.7044806  -1.4135427   0.3861026  -0.91807747  0.9130914\n",
            " -2.687796  ]\n",
            "Sion: [-0.5962978  -1.4888341  -1.8729564   0.13131526 -1.0189811   0.847895\n",
            " -0.43030843 -2.004374    0.25785244 -0.96743166 -0.3343426   0.6627506\n",
            "  0.2535208   1.7767338  -2.435339    2.9607584  -0.9708101   1.9693478\n",
            "  0.98858815 -0.60376495 -1.4192265  -0.9389899  -1.2922118  -0.1752939\n",
            "  0.6362471 ]\n",
            "Sivir: [-2.4313116   0.54022294 -1.2562344  -1.5190105   1.0662246  -2.4200785\n",
            " -3.0600548  -1.7548119  -0.05952622 -2.4201162   0.7914019   1.0377203\n",
            " -2.7587945  -0.92888856  0.90097153 -0.44626608 -3.487202    0.4731734\n",
            "  1.0603287  -0.4313647   0.26183328  3.0544446   0.4346311   2.9375072\n",
            "  2.5851357 ]\n",
            "Soraka: [ 0.17947784  1.9721216  -1.262501    1.1813378  -0.6410689  -1.124413\n",
            " -0.08926275  0.73421603 -0.4317003  -0.601362   -1.048818    1.412461\n",
            " -1.2831173  -1.4267278   3.3372304  -5.265687    0.7031549  -2.983851\n",
            " -3.0888257  -0.7798124   0.54680103 -0.0945835   1.6388708  -0.05654838\n",
            "  0.7618991 ]\n",
            "Teemo: [ 3.2829304   2.650483   -0.64491504 -3.8599157  -0.52895653 -1.044689\n",
            "  0.3517917   2.741805   -1.0387206   1.703971   -0.26062852  2.156171\n",
            "  2.5352523   2.5358114  -0.10384583  1.502239   -0.15546963 -1.3211305\n",
            " -2.534758    1.4390374   0.41168728 -1.0634652   0.95501715 -1.4642209\n",
            "  2.7149165 ]\n",
            "Tristana: [-0.5762744  -2.1226344   1.1453891  -1.006404   -2.3882346  -0.8737704\n",
            " -0.7813761   3.3030696   1.8329314  -0.5463268   0.8520562  -2.0427759\n",
            "  0.60047317  0.15405692 -2.2742584   0.77307594  0.9060058  -0.6583255\n",
            "  2.7132583  -1.35847    -3.0781593  -2.7322428   0.6904292  -2.0852451\n",
            "  2.2421534 ]\n",
            "Warwick: [ 2.6911466  -2.2699556  -0.5747792  -2.3358033  -3.755       1.6974453\n",
            " -0.53466034 -1.712061   -1.2993268   0.49114645 -0.5566422  -2.3036647\n",
            "  0.8504519   1.425242    1.1001074   1.1079919   2.1425822   2.3526266\n",
            " -1.6383227   2.5321798   0.9397149   1.4441279  -0.16711043 -0.81672573\n",
            " -0.481809  ]\n",
            "Nunu: [ 1.6483786  -0.63659763  3.0422893   1.6755313  -1.5882753   1.3405125\n",
            "  1.2475597   2.4267986  -2.542494   -3.0134454   0.8935156   0.5310764\n",
            " -0.8245551  -1.4417688   2.3372035  -3.222156   -1.0274718  -0.352658\n",
            " -1.0704077   1.4262397  -0.28812084  0.87921065 -0.21134691  2.6161876\n",
            "  4.0897546 ]\n",
            "MissFortune: [ 0.39756694  0.36751336  1.7296599  -0.17782703  1.9152094   1.3252314\n",
            " -1.0018291   1.3368022  -0.8117403   0.9485312  -0.6390433   0.7742297\n",
            "  0.77102745 -1.1740663  -0.12855087  0.27973825  1.1199158   0.73201203\n",
            "  1.7308043  -0.24325196 -0.35747015  0.366626    0.2984726  -1.5366787\n",
            " -0.2627385 ]\n",
            "Ashe: [-0.82396865  0.30648398  2.1736007  -0.9945128   1.4003435   1.7380363\n",
            " -2.1321995   0.14892086 -0.9646067  -1.8423054   1.2078363   1.1868253\n",
            " -2.28324     1.6759565  -2.731828   -2.3352773  -2.6395512   0.17724988\n",
            "  1.3111078  -1.5216215   0.3814833   0.05781314 -2.6647537  -1.5384542\n",
            " -3.944613  ]\n",
            "Tryndamere: [ 0.876325    0.4100223  -1.5934657   0.4358495  -1.4486092  -0.62236303\n",
            "  1.3928614   0.5837974  -0.5433193   0.98860586  2.8343315  -1.5993104\n",
            " -1.5906929   0.64339554 -0.12817974 -0.35903603  3.4797502   0.34837148\n",
            " -0.05034344 -0.61995006  0.4668528   2.5128963   0.66137075  2.8925786\n",
            "  2.1780467 ]\n",
            "Jax: [-2.8288107   1.9787673   1.7340679   0.6243924  -0.9641129  -0.51460654\n",
            "  0.04539183 -1.3280103   0.47586924 -1.3652601   1.1442667  -0.842199\n",
            " -1.6379781   0.8712932  -1.3224891  -1.9681333   1.2954738   2.7115614\n",
            " -0.55595547 -0.7918905  -1.3024933   1.3812033  -0.8468034  -0.30184346\n",
            "  2.3962207 ]\n",
            "Morgana: [ 2.0752802   1.0141436  -1.248688   -0.01905316 -1.7879131   0.7008188\n",
            "  2.7249653  -2.6822662   1.7488751   1.7674123  -0.50117713 -1.081984\n",
            " -1.1754159  -0.52396965  0.23870742 -0.5865563   2.0728655  -0.38541016\n",
            " -1.7251611  -0.6936949   0.51275575 -1.3054646  -1.2746471   0.7314667\n",
            " -1.271531  ]\n",
            "Zilean: [ 1.8827764   0.49101344 -0.13177529  0.35604188 -2.354789    0.8551\n",
            " -2.600745    1.6518701   0.9525194  -2.1358263  -1.8434503  -2.5812075\n",
            " -0.8928216   0.1354355   0.94458157  0.3791822  -1.9063537   1.2228206\n",
            " -1.717633   -3.8886917   2.366564   -5.715525   -0.5611974   1.0252486\n",
            "  1.2565805 ]\n",
            "Singed: [ 0.01952449 -1.7016251  -1.3530366   1.198067   -0.2141733  -2.19512\n",
            "  2.0748553  -0.48703226  0.1454261  -0.29403192 -1.2313294   1.529626\n",
            " -0.538704    2.9338725   0.20866495  0.40187544  1.6030898   0.6556611\n",
            "  0.08903595 -1.4185529  -0.31124482  1.0808029  -1.4264266   0.34484684\n",
            "  1.1003418 ]\n",
            "Evelynn: [-1.5579058  -1.3027748   2.3934534  -1.8958074  -0.05930991 -0.8261636\n",
            " -2.6314435  -1.3114128   4.24688    -1.7169914  -0.98894346 -1.5423579\n",
            " -2.9204667  -3.3901033  -1.9123644  -1.4537418  -2.75258     1.9918052\n",
            " -2.4734352   0.8584373  -1.3243468   1.1981376   1.1044734   1.0071259\n",
            "  0.7428842 ]\n",
            "Twitch: [-1.371395    2.0007765  -0.9787874   1.4596305   1.1091356  -0.05345043\n",
            " -2.088993   -0.4219585   3.4627488   0.43364817  0.6872838  -0.70339954\n",
            " -0.7825192  -1.5924644  -3.6576395   1.229371   -2.3167408   1.8469213\n",
            "  0.92424804  0.10067026 -1.7152421  -2.0992205   0.7525907   1.7262474\n",
            " -3.1878767 ]\n",
            "Karthus: [-1.0381993   2.170753   -0.7403681  -0.6874082   0.36192316 -0.9246619\n",
            " -0.79423547  4.0069237   0.38190174  2.994222    1.163182   -0.9343508\n",
            " -0.06684704  1.9771521   0.31815264  1.7490112  -1.9482591   1.4897233\n",
            "  0.6903684   0.9861865  -3.0475645   0.05816142 -3.3077064  -2.5710828\n",
            "  2.1260967 ]\n",
            "Chogath: [-7.9233509e-01 -1.1751406e+00 -3.9466327e-01 -2.9242594e+00\n",
            "  8.6650389e-01  2.4430654e-03 -1.0845312e+00  1.1832284e+00\n",
            "  1.4096158e+00 -1.5016024e-01 -1.6244417e+00 -5.5088520e-01\n",
            "  2.1094127e-01  4.2789090e-02 -9.9275678e-01  6.9298011e-01\n",
            " -1.2443653e+00  2.4356329e+00  1.1080830e+00  1.4573135e+00\n",
            "  4.3362673e-02  1.2115317e-01  3.3390484e+00  1.5077715e+00\n",
            "  1.5454073e+00]\n",
            "Amumu: [-0.12278977 -0.81140774 -0.58893514 -0.7090092   1.0822296  -4.3183556\n",
            " -0.68569237 -1.3637803  -0.62648755  3.4801936  -0.826924   -0.25425914\n",
            " -1.1718974   1.2108345   0.22578798  1.1795224   0.93393785 -2.9258623\n",
            "  1.1769081  -0.53898567  3.5844114   1.1290134   0.9133167   1.0444775\n",
            "  2.719051  ]\n",
            "Rammus: [-1.8116596   1.3982534   1.6394291   2.5555112  -0.28051287 -3.1341164\n",
            " -1.0851285   0.3157498  -0.4657466   2.0944214  -0.8553484   2.0039558\n",
            "  1.6576482   1.0687653   1.6808078   0.4367833   3.736299   -1.8706038\n",
            "  0.957008    1.7791557  -1.9598702   0.1282631   2.033585   -3.5425959\n",
            " -2.9379954 ]\n",
            "Anivia: [ 2.2341483  -2.3961701   0.9140934  -2.9632008  -0.92018914  1.3976291\n",
            "  0.10070959  1.4233066   0.07207917 -1.7069285  -1.9373723  -4.0968776\n",
            "  1.8124342  -0.5636393  -3.2548947   0.44940063 -3.0404656   0.5495269\n",
            "  0.32087594  2.6773431  -0.83247834  0.42565936  3.1308732   1.4788429\n",
            " -0.26627448]\n",
            "Shaco: [-0.02627329  1.0122452  -0.89021814 -3.5737035   0.50188446 -1.5002458\n",
            "  1.007554    0.744351    0.40689385  2.179648   -2.5140169  -1.2090366\n",
            "  1.1783825  -1.7246783  -1.0140212   0.52184147  0.4975276   1.6121435\n",
            " -0.03769279  0.7464868   0.6567382   0.5858311   2.3767605   0.05831859\n",
            " -3.5357647 ]\n",
            "DrMundo: [-0.1230858  -0.90395284  0.94067824  0.36783275 -1.8986108  -0.11338644\n",
            " -1.7575006  -0.30112764  1.5248678  -1.7045858  -1.7653216   0.50055766\n",
            " -1.6819432   1.644902    0.51257133 -0.02752433  2.966063   -1.8976256\n",
            " -0.6472224  -1.8969139   2.6161091  -0.48066184  1.2883937   1.0801615\n",
            "  2.1349552 ]\n",
            "Sona: [ 0.83715147 -2.521241    2.0112157  -1.1652666  -1.9796638   1.2632596\n",
            "  2.9086015  -0.5000552  -0.56341314 -0.8070208  -0.1897638   1.3361275\n",
            "  0.46395394  0.44556153 -1.495814    2.3573549   3.2549758   1.019637\n",
            " -1.8162963   2.4235816  -0.76451504  0.38739944  3.0831215  -0.30614537\n",
            "  1.7775918 ]\n",
            "Kassadin: [-1.2761563   1.5596194   2.0544324   0.27863875  0.6337035  -3.1308222\n",
            " -0.11592942  0.40549266 -3.0302448   1.3585864  -1.8910365   3.760912\n",
            "  1.0143224   0.03851642  2.1242678   2.8930557  -0.48662767  0.6774469\n",
            "  0.08149301  2.1649923  -0.76621413 -0.02293024 -0.1939994   1.2208253\n",
            "  0.51673865]\n",
            "Irelia: [-0.42023045 -0.19304895  1.8497097   2.4813824   0.22128169 -2.0925803\n",
            " -0.90799767 -3.0946968  -2.0946438  -0.05501991 -0.04611093  2.1290405\n",
            "  0.27798176  2.25692    -0.6563274  -0.4112822  -0.633808    0.78893757\n",
            "  1.9277035   2.1670575  -2.3435683   4.005667    0.55764717 -0.28461343\n",
            " -1.2953737 ]\n",
            "Janna: [ 0.5709343  -1.239003    0.02972276  4.031412   -2.529306   -3.7711632\n",
            "  0.7275985  -1.5948663  -1.1862074   0.0257972  -1.36944    -1.4995385\n",
            "  0.48230886 -3.6611571   1.6264656  -3.0438826  -0.49994415 -0.26776463\n",
            " -0.14982823  0.10796993  0.73931223 -1.1897135  -0.7403087   0.7174505\n",
            "  2.2896943 ]\n",
            "Gangplank: [ 2.1627937e-01  2.8111546e+00  6.0515237e-01  4.1169670e-01\n",
            "  2.5199554e+00  1.2906295e+00 -1.2585905e+00  1.7436361e-01\n",
            "  1.8314379e-03 -3.6033741e-01  1.4781656e+00 -1.1449753e+00\n",
            " -2.5619343e-02  2.0065782e+00  5.3976333e-01  1.5942091e+00\n",
            "  5.7802898e-01  1.9046594e-01  4.0842075e+00 -1.8155658e+00\n",
            "  7.3443687e-01 -7.8542209e-01  3.2251988e-02  1.4150803e+00\n",
            " -2.2501924e+00]\n",
            "Corki: [-0.82055545  2.2280915   1.7883661   1.3603612   2.1478927   4.7219415\n",
            " -2.5461574  -2.555174    2.169387   -0.19184512  0.51844543  0.5971053\n",
            " -0.05742318 -0.34501016 -0.29777908 -2.9309723   0.6583369   1.6190044\n",
            "  0.7465825  -2.5159469  -1.2728542   2.2131612  -0.09805717 -1.6339996\n",
            " -4.254163  ]\n",
            "Karma: [ 1.4867358  -1.0403516  -0.5422345   0.00750519  0.713348   -0.23771514\n",
            " -1.5211658   0.3436253   1.7852103  -5.0736494   1.604667   -1.670502\n",
            " -1.3624228  -1.6907728  -2.6731896  -1.4829863  -2.3897567  -1.5289277\n",
            " -2.6773612  -0.9887313   1.1604319  -2.3251855   0.37097505  2.7236207\n",
            "  0.5156894 ]\n",
            "Taric: [ 2.510557   -2.7595353   0.16409206 -0.06873492 -3.7036276   1.5966411\n",
            "  0.33677742  2.4964764  -0.19576707 -4.1250105   0.4707702  -3.646053\n",
            "  1.8082528  -2.1141186   0.8025206  -0.88201296  0.8320062  -1.9695493\n",
            " -2.2461512  -0.5034485  -1.0314465  -2.0724874   3.239762   -0.33121195\n",
            "  2.0361073 ]\n",
            "Veigar: [ 1.5640635  -1.1342367  -2.839848    0.4669137  -1.0726004  -0.44927707\n",
            "  2.1716661   2.4705682  -0.51585823  1.3018135  -1.6971622  -2.3459604\n",
            "  1.1285242   0.97073627 -0.20888118  0.4298212  -0.38511312 -2.1035864\n",
            " -1.5879097   0.03903908  0.28753555 -1.8220935   2.2599826   0.2974678\n",
            "  1.6381413 ]\n",
            "Trundle: [ 2.2121062  -0.5762948   0.5211638  -4.8650413   0.23261741 -0.29148233\n",
            "  4.2088466   3.8100111  -0.11983085  1.3532879  -1.5236785  -1.1158327\n",
            "  1.0876892   2.859655    0.40939367  1.049677   -0.6253361   1.154221\n",
            " -1.3184619  -1.9049349   0.03393137  0.8058722  -1.8548615   0.02489314\n",
            " -0.5098181 ]\n",
            "Swain: [ 1.0273483   3.1224718  -1.085144    0.6288931  -1.1989149  -3.765329\n",
            "  0.9982674   1.8460095   2.3977573   2.0591486  -0.05735436  1.3942269\n",
            "  2.6447232  -0.2201744  -0.70525473  2.3135076   0.26138005 -2.1184034\n",
            " -0.81531024  0.5318496  -0.49404874  0.8161159   1.7560769  -1.6121584\n",
            " -1.3254203 ]\n",
            "Caitlyn: [ 0.22966012  2.203148    0.7451387  -0.7384872   1.0679733  -1.4551528\n",
            " -0.9082349   2.2982576   0.10377254  1.3089458   1.2526342  -0.7427038\n",
            " -2.0109515  -0.14876956  1.697829    1.3896462  -2.1915116   1.6292481\n",
            "  2.2633414  -1.3118726  -0.77875704 -2.389535   -1.213566   -1.1259686\n",
            " -0.714335  ]\n",
            "Blitzcrank: [-0.6676067   1.062049   -0.10234765 -1.0935014   1.789255   -0.97640336\n",
            "  0.0939303  -1.5179318   0.7639301  -0.8480683   0.41766882 -1.8953464\n",
            " -1.4961573   1.5944321   0.2932995  -0.8377729  -3.5726686  -0.03828568\n",
            "  1.7680365  -2.2996917   1.5237286  -3.2063928   0.7171666   0.85584116\n",
            " -0.65279645]\n",
            "Malphite: [ 0.92537    -0.59943837 -0.02690525 -2.2017982   0.61447555  0.0686912\n",
            " -1.2935151   2.2826736   0.30425358  1.4528741   1.2093908  -0.9555559\n",
            "  0.25292358 -0.29140076  2.1373763  -0.14815497  0.50266373 -2.0568092\n",
            "  0.18614307 -2.0561812  -0.880046   -3.8162158  -1.8760657  -1.0791591\n",
            " -1.1003993 ]\n",
            "Katarina: [-1.5454731   1.8098794   0.9954867  -1.1609061   1.6962825  -3.6305027\n",
            " -0.5724475   0.5670981   0.6906337  -1.2552658   1.9312944  -0.24001901\n",
            " -0.0523738  -0.41040027 -2.7627892   0.10353138 -1.069933   -3.481598\n",
            " -1.1617768  -1.0013033  -1.18965     0.796727   -1.6294615   1.9091709\n",
            "  2.0291    ]\n",
            "Nocturne: [ 0.86275125 -1.8937343  -0.83486927  0.64795786 -1.3212242   1.0959135\n",
            "  1.2850624   0.2427143   1.2754055   0.2239533  -1.4864852  -3.4361188\n",
            "  1.0414584   0.7041451  -0.5671599   0.14707474 -0.5882192  -3.7593715\n",
            " -1.2970086  -0.00527103  0.7186712  -0.3948411   1.0620561  -2.6075366\n",
            "  1.8290409 ]\n",
            "Maokai: [-0.07245889 -1.9510144   2.01636    -2.2250054  -1.329548    1.0596535\n",
            "  0.05909301 -0.10599037  3.2808409  -1.8100901  -2.3794124  -2.33186\n",
            "  0.5500201  -0.01509773 -2.5476017   0.94436294 -1.8206105  -1.6848699\n",
            "  0.1654643  -0.925506    1.5133562  -1.9113799   2.2468648  -2.7186825\n",
            " -0.25876048]\n",
            "Renekton: [ 0.05141418 -1.0924699  -0.12609163  0.6687486  -0.2869669   2.012443\n",
            "  1.9152154   0.00990661  0.6566968  -0.7492455   2.0214345  -0.27517375\n",
            " -3.0837305  -0.5047507   1.6175352  -3.3750393   1.717089   -0.54457694\n",
            "  0.10365275  0.2859694   0.12058993 -0.04562218  0.24899584 -1.3562827\n",
            "  0.22990663]\n",
            "JarvanIV: [-1.2849919  -3.0355449   0.5172944   1.3842777  -0.33908394 -2.1342287\n",
            " -0.32606396 -0.80894506  0.39371023 -0.9146019   0.2289162   1.2306039\n",
            "  0.10721426  1.9708759  -0.40733162  1.3562804   0.829961    0.18325973\n",
            " -0.26013526  0.58421797  0.64400214  1.9481514   1.4919552   4.329325\n",
            " -1.3730325 ]\n",
            "Elise: [ 0.53335524 -1.6145175  -1.2262094  -1.3046844  -0.703938    0.6366524\n",
            "  1.8242689   0.82158345 -0.70538986 -1.7966057  -0.40620223 -1.6591947\n",
            " -0.3628878  -0.13587642 -0.16800688 -2.5368335   0.3431228   1.7537932\n",
            " -0.3148083   0.9417168  -1.202784    0.89957166 -0.8220142   0.6785677\n",
            "  0.08259616]\n",
            "Orianna: [-0.47688824 -0.9216845   1.479519    1.2994438   1.6951946  -0.34678027\n",
            "  0.943113   -2.0148032   3.5665348   0.07802546 -3.4082701  -0.7958108\n",
            " -0.44755483  0.6697929  -0.91841763 -1.7794547   2.25402     0.0620927\n",
            " -0.7166684  -0.76830643  1.2575835   1.7757236  -0.23397142  0.32096407\n",
            " -1.7219119 ]\n",
            "MonkeyKing: [ 0.9449262   1.8443193  -0.58253     0.97203016 -1.0804278   0.2712858\n",
            " -0.52695894 -2.8864248   1.8423017   2.5412183  -2.2016675   0.09308672\n",
            "  1.2459985   2.2381468   0.80185145  1.9034797   0.9853373   2.1068656\n",
            "  1.318412    0.95170885  3.6015031   4.424927   -1.2526481  -3.355043\n",
            " -2.5917978 ]\n",
            "Brand: [-0.9225026  -0.08194908  0.36856318  0.2915295   1.2974994  -1.2816821\n",
            " -2.5654902   0.15074067  0.8673903  -0.8426256   0.9311025  -1.5564535\n",
            " -0.26687336 -1.5458927   0.9179457   0.9114938  -1.986704   -0.54482806\n",
            "  2.1385133  -2.2779686   0.46208337 -3.3876824   0.631483   -1.2842546\n",
            " -0.8583147 ]\n",
            "LeeSin: [-2.6378233   3.5411627   1.5325565  -0.48816562  1.8855188   2.9522407\n",
            " -3.439417   -1.1450235   1.0715245  -0.48978424 -0.38342294  1.0882757\n",
            " -2.253583   -0.44020796 -0.24815719 -3.0396264  -0.48573825  1.7480848\n",
            "  2.6924417  -0.50456446 -2.7629154   0.59812397 -0.29449692  0.3320627\n",
            " -1.6285915 ]\n",
            "Vayne: [-0.47797346 -0.18977153  0.55429405  1.7308688   0.18406539  0.9291792\n",
            "  1.1192191  -2.6271904  -1.2842311  -0.6920629  -0.31078032 -0.7764818\n",
            " -0.8599005   0.02765859  1.0786513  -1.8908741   0.9203627   1.6665641\n",
            "  0.6056062   2.394646    0.15350822 -0.3152684  -0.43901557 -1.2280598\n",
            " -3.4452398 ]\n",
            "Rumble: [-1.1320063   2.4555418   2.8226032   3.377145    2.1257887  -1.155753\n",
            "  0.2685615  -5.397883    0.1773399   0.2375629  -0.17754099  5.7962294\n",
            " -1.7476248   0.32179844 -0.327836    0.29057068  0.26234517  1.9919435\n",
            "  3.4657652   1.1257     -0.42865327 -0.8890608  -1.4863492  -1.1707768\n",
            " -0.23905937]\n",
            "Cassiopeia: [ 1.7920239  -0.20980898  0.34556374  0.650574    0.04549293 -2.4222221\n",
            "  1.9224747   0.7868188   0.5387368  -1.400472   -1.7830533  -2.5006335\n",
            "  2.365797   -1.5985026   0.44479746  0.3531802   0.57107085 -2.710188\n",
            "  0.60355353 -1.4204957   2.7746522  -3.3848538   0.3925798  -0.16943914\n",
            "  0.9362807 ]\n",
            "Skarner: [ 0.15587015  0.37091175  0.32022956 -1.9675788   2.6943984  -1.6216046\n",
            " -2.4895763  -2.2356536   1.2398643  -2.0279047   1.4353671   0.83209324\n",
            " -3.2628987   4.9961724   0.9192384  -0.62967724 -2.217185    1.5771343\n",
            "  3.119992   -3.231293   -1.7633255   2.3738086  -1.6919612  -1.4431351\n",
            "  1.1187508 ]\n",
            "Heimerdinger: [ 0.25488096 -3.4202805  -1.0375167   0.42562774 -2.395837    1.5301548\n",
            "  1.5650644   0.63170046 -0.7020099  -1.1842602  -0.8723247  -0.6690606\n",
            " -2.654406   -3.8463395  -0.03917451 -0.563499   -0.1285902  -1.7248824\n",
            " -2.21891    -1.2807817  -0.6892236   0.21528597 -2.3458922   1.2747496\n",
            "  2.2966597 ]\n",
            "Nasus: [-1.4015709  -0.65931916 -2.9591782   1.1871173  -0.28552046 -2.0840619\n",
            " -0.5641209  -1.3699756  -1.0882225  -0.07384601  0.37307107  2.105838\n",
            " -0.18548124 -1.6472511  -1.4095682   0.54735994  2.9803193  -0.32564354\n",
            "  0.68049806 -0.9859809  -0.0818855  -1.724383   -0.6983427   3.4813821\n",
            "  1.469006  ]\n",
            "Nidalee: [-2.9383943   5.2005916   2.3155053   1.4040159   1.9939599  -1.0363997\n",
            " -2.5932655  -3.5242908   1.2852584   0.76443917  1.9809664   1.658064\n",
            " -1.6006688   1.3998939   0.2869673   0.7443027  -2.69891     1.5415765\n",
            "  5.775467   -1.60352    -0.84175825  0.81460893 -3.1266522  -2.0103414\n",
            " -6.0768313 ]\n",
            "Udyr: [ 0.15082653  0.22101764 -1.4507723  -0.7190986   0.7417586  -2.3893406\n",
            "  0.49981546  1.8997949   2.7062361  -2.3917763  -1.2266285  -1.6465921\n",
            "  2.3189619  -3.3242955  -1.8901175   0.8587139   0.08765088  0.2919127\n",
            " -1.0784986  -0.74839836 -1.7364739  -1.8506472   4.234329    0.42958993\n",
            "  0.9726496 ]\n",
            "Poppy: [ 0.4029221  -1.9216896   1.3120419  -0.36750692 -0.993272    2.2053838\n",
            " -1.1628073  -3.217054   -0.9752217   0.45667043 -0.4077336  -0.1066433\n",
            " -1.8112124   0.7305779   0.76731014 -1.0779866   1.0199242  -2.4049284\n",
            " -1.7197367   0.10836296  0.8152026  -2.3173363   1.8605611  -0.93785506\n",
            " -1.2071244 ]\n",
            "Gragas: [-1.1139199  -1.2705063  -1.4228599   1.6452385  -0.7695577   0.47266755\n",
            " -0.5960108   1.0586973  -4.222459   -0.05949539  0.2089032   1.980525\n",
            "  1.1555504  -2.40944     0.63908046 -1.2489054   2.1429622  -2.1150165\n",
            "  0.83863485 -2.1291366  -0.8632995  -0.32277498 -0.7408446   3.415424\n",
            " -2.0589411 ]\n",
            "Pantheon: [ 0.7936101  -2.131506   -1.2005284  -1.1230476   1.1443743   4.5137343\n",
            "  0.63954234  1.9611667  -2.6473951  -1.977817   -0.01063079  0.9901182\n",
            " -0.759506   -0.7099654   0.18393159 -0.27734295 -1.7840217  -1.9297962\n",
            " -1.1985793  -1.0606508  -0.35430005 -0.15163948 -0.75533444  0.17869125\n",
            "  0.2115983 ]\n",
            "Ezreal: [-2.9240193   1.5407901   0.93565685  1.6037204   3.741099    2.479012\n",
            " -0.94307846 -3.03303     0.16094238  1.6887313   1.0440817   1.9856157\n",
            " -1.1642028  -0.3697803  -4.647872    2.1300447  -0.6216268   3.5524788\n",
            "  2.3557181   1.6388968  -1.1437845  -0.10678229 -3.5539646   0.24825753\n",
            " -1.0236033 ]\n",
            "Mordekaiser: [-1.983494    0.42591232 -0.69876593 -1.9882019  -0.13088343  2.6973405\n",
            " -0.28898227 -2.9339948   2.2422576   0.2441917   0.45133853 -2.3982854\n",
            " -0.6778755   0.51943797 -2.9723978  -2.7593355  -3.5611653   1.815503\n",
            " -0.84039223 -0.49976283  1.5334039   0.71005446  1.7989738  -0.57522166\n",
            "  1.6044562 ]\n",
            "Yorick: [-0.68078077 -0.6126472  -1.5971829   0.53490317  0.18552034 -2.2508006\n",
            " -0.5544162  -1.7969328  -1.7479268   1.0838524  -1.5440708   3.4769802\n",
            "  1.5631868  -0.5764295   2.5065808   2.5510552   0.05889135 -2.5595093\n",
            " -1.2323965   2.2810385   1.9734995   1.6842128  -1.8259503   1.623534\n",
            "  1.1965463 ]\n",
            "Akali: [-1.6836464   1.7562785   0.53293234 -1.6696534   3.0700245  -1.5069885\n",
            " -2.702384   -1.2101529   0.7082575  -1.2134188  -0.3374047   0.31053963\n",
            "  0.40334716  1.6082665  -0.25589335 -1.3276402   0.03964241 -0.33224446\n",
            " -0.35852277 -1.9245601   0.973564    1.2204388   1.2318401   0.88641393\n",
            "  0.49194917]\n",
            "Kennen: [-0.15699321  1.0110748  -0.42823863  2.4685657   0.03619008 -1.1550171\n",
            " -0.6489049   1.2095257  -3.369662    0.14107482  2.2965448   3.4179626\n",
            "  0.603767    0.60369605  1.9333427  -0.06099966  3.9224133  -2.047923\n",
            "  2.9306393  -1.4445854  -3.8536098  -1.1413214  -1.6732512  -0.8255757\n",
            "  0.5748541 ]\n",
            "Garen: [ 2.1251163   0.47900003 -1.5457301  -0.36187512  0.30230796 -1.5897924\n",
            "  2.2251265   3.680273   -1.9168837   2.1937735  -1.9743066  -1.8573835\n",
            "  1.8322479  -0.11326882  2.0266726   0.13553403 -0.17671944 -0.968298\n",
            " -2.0608082  -1.9058881   1.1434854  -1.2498353  -0.21995208 -0.9618949\n",
            " -1.2843733 ]\n",
            "Leona: [ 1.903938    1.8762879  -0.13152103  0.7993252  -0.5771135   2.5377007\n",
            " -1.9348656  -0.00881988  0.40980583 -0.9334151   0.63912773  0.09580098\n",
            " -1.0618312   1.4161651  -0.26853743 -0.76689786  1.2420038  -0.35959136\n",
            " -1.3294622  -0.33677852 -1.6480942   0.4069739   3.5998375  -1.210472\n",
            "  0.43813533]\n",
            "Malzahar: [ 1.3213632  -1.4617151   0.7644616  -1.3306228   1.3307024   1.0874798\n",
            " -1.4242988  -2.208178    1.5433133   0.12211209  0.2562024  -0.97277385\n",
            " -0.5601659  -0.59741247  0.20707877  1.6663085   0.9658013  -1.4520702\n",
            "  2.497675    2.7999003  -0.9468803   1.274207    0.9675374  -0.5923361\n",
            " -2.205881  ]\n",
            "Talon: [-3.5771148   0.5260206  -1.969896   -0.07658576  0.51867205  0.20827001\n",
            " -1.0762813   2.8248968  -0.14712532 -0.31374198  0.4966766  -0.48442915\n",
            "  0.2711851  -1.188335    0.7647583  -0.714269    1.1263459   0.4892328\n",
            "  1.0648996  -1.4413031  -3.1827605   1.2809982  -0.33771855  0.37992907\n",
            " -0.24226087]\n",
            "Riven: [-0.6966875  -2.0212977  -0.40527648  0.9929766  -2.2366934   2.6070936\n",
            "  1.5193838   0.02451536  0.59500694 -0.64506924 -1.1069629  -2.7961311\n",
            "  0.67891806 -2.8707743   2.081447   -2.5120282   2.0873477   3.2805204\n",
            " -1.6346712   0.3392932   2.2809784  -1.9850165  -1.1115814  -0.3067537\n",
            "  0.01709521]\n",
            "KogMaw: [ 0.9763048   0.5398039  -0.83503985  0.42599502 -0.83239615 -0.00918195\n",
            "  0.5648207  -1.4273926   1.4803793  -0.502861   -0.03593559 -0.29306734\n",
            " -0.7214487   0.14637755 -0.19291885  2.6022272   0.9051615   3.153387\n",
            " -0.17159972 -0.8454877   1.281688    1.6633395   1.8360816  -1.574491\n",
            "  1.0746323 ]\n",
            "Shen: [ 0.89525807 -0.9944698   0.17147784 -1.6154683  -1.6893345  -2.4810412\n",
            "  1.8548003   2.274517    1.9656336  -1.9514831  -1.3312216  -1.301309\n",
            "  0.02278499  2.9925148  -0.5960765  -1.2554364  -0.6880171  -0.53905874\n",
            " -0.84691584  0.3778899   0.09167412 -1.8575729  -0.2770432  -3.179814\n",
            "  0.6822072 ]\n",
            "Lux: [ 0.20027204  0.4459061   0.58287984  1.6357319  -0.44241357 -1.335187\n",
            " -0.46055198  0.48447204 -0.17364761  1.9849137  -0.6039139   1.48312\n",
            " -2.4433017  -0.41979858 -0.7363276   0.22217906 -0.92442757 -0.77700454\n",
            "  1.1102589  -1.0334041   0.88889045 -1.6262873  -2.0434744  -0.3069279\n",
            " -2.111064  ]\n",
            "Xerath: [ 1.5502597   0.00785715  2.6589763   1.226487   -0.02338248  4.638797\n",
            " -0.20111403 -1.7779967   0.09078553 -0.21666835 -0.2145205   1.263222\n",
            "  0.82834244 -1.3484075  -2.3015082   2.6872594   1.2717797   0.55800635\n",
            "  1.3077494  -0.9928626   0.30424345 -2.4947681  -0.7900839  -0.9969398\n",
            " -0.35940373]\n",
            "Shyvana: [-1.9720275   0.92172784 -0.53203326 -2.2655814  -0.3046007  -2.3817365\n",
            " -1.2193409  -3.4155521  -1.0021195   0.8579504  -3.5267498   0.7639483\n",
            "  1.5410347   2.9160025  -1.346176    0.5272316  -0.29653183  1.1189975\n",
            " -0.12134955 -0.75243485  1.5939974   0.99931574 -1.4901744   0.79718363\n",
            "  1.1886154 ]\n",
            "Ahri: [ 0.21835074  0.10792125 -1.5027373  -1.5519779  -1.9221251   0.79860646\n",
            " -0.962578    1.2620295  -1.3088884   2.008573    0.33351886  0.35466313\n",
            "  0.39708763 -2.050355   -0.26204702 -0.90991735 -1.1681821  -1.3255469\n",
            " -2.14299     2.1791677  -0.50654465 -1.0968549  -0.62760925 -0.66334677\n",
            " -0.44575506]\n",
            "Graves: [-0.31136474  3.4734058   3.2180421   0.24473736  1.9708271   1.6735848\n",
            " -1.875007   -1.2988756  -0.53836167  1.8372816  -0.18804805  0.5669295\n",
            "  0.3283634   1.3853208   2.534439    2.6478837  -3.2506223   0.18767771\n",
            "  3.4675903   1.5790555   0.5702755  -0.47044426 -0.02189979  1.2432891\n",
            " -1.8288245 ]\n",
            "Fizz: [ 0.660455    0.45173874 -1.5349714  -0.62597203  2.8548205   2.8787978\n",
            " -0.4482346   2.3936949   2.9735448   0.8170673   0.94606024 -0.49917105\n",
            "  1.1397127   0.9826893   1.0508093  -0.04603872 -0.6475432  -1.9795952\n",
            " -1.7420735   1.851547    0.7262453  -0.46638754  0.6302913  -0.05908997\n",
            " -0.29893056]\n",
            "Volibear: [ 0.16098355 -1.4609524  -2.16484    -1.0427926  -1.0074967   0.5884494\n",
            " -0.5874589   0.6562375  -0.6665452   1.1588856  -1.3194385  -2.45464\n",
            " -0.10380253 -1.0218465   1.7056915   1.2494944   0.38161868 -1.6262211\n",
            " -0.26456392  0.43293053  0.88151115  0.17297544  2.5417569  -0.55337787\n",
            "  2.1084738 ]\n",
            "Rengar: [-0.98077255  1.2424809  -0.1758821   1.8605341   0.00680047  0.86016726\n",
            " -0.50990623 -0.9788521  -0.25049558  1.1782165   1.192072    0.55270064\n",
            " -0.49975634 -0.6042619   1.1343395  -1.5035247   2.391144    1.7568367\n",
            "  1.7743316  -2.2726443  -0.5057501   4.540127   -3.659772   -2.9346945\n",
            " -2.9421823 ]\n",
            "Varus: [ 0.30699936  0.95037264  0.66123116  0.5173649   3.2027862   1.2296792\n",
            " -0.93973047 -0.2891651  -0.66472846  3.753943    0.3020943   1.5932447\n",
            "  0.30583915  0.28014824 -0.47751752  0.1593908   0.03652589 -0.92717516\n",
            "  1.9098538  -0.24170187  0.22456783  2.8697941  -1.8096255   0.26731688\n",
            " -2.0635138 ]\n",
            "Nautilus: [ 1.0536567   1.1353567  -0.40189803 -0.6773061  -1.2551868   0.8465326\n",
            " -0.7427081  -0.95077     0.6066559   0.27369624 -0.8202871  -0.32736695\n",
            "  1.7282917  -0.4979237  -2.1126227  -0.37232527  0.07371675  0.5419339\n",
            "  1.6334834  -1.2820234   0.08689523 -2.3755045   0.35299778  0.50187975\n",
            " -0.00564244]\n",
            "Viktor: [ 0.13403673 -1.2974997   3.5372176  -1.957384    1.7647092  -0.44021642\n",
            "  1.3110919   0.4705349  -0.07164224 -1.5976849   0.9184254   2.043773\n",
            " -0.9943403   2.4919996   2.9268854  -1.021346    1.3736469  -0.24449311\n",
            "  2.0436206   1.9019345   1.5643091  -1.2765689   2.1230056  -1.2264168\n",
            "  1.3612427 ]\n",
            "Sejuani: [-0.16814342  1.6795338   0.19658624  2.9225454   0.9425001  -0.48897108\n",
            " -3.5476549  -1.0135098  -1.1628487   0.2604426   1.9881207   2.0095885\n",
            " -1.0949417  -0.56967515  1.5018015  -1.2754228   2.1210067   1.113884\n",
            "  2.1169698   0.16822484 -1.8138144   1.3321447   1.505261   -1.2088332\n",
            "  0.54690313]\n",
            "Fiora: [ 0.06432977 -2.4279318   1.0088232   0.22517931  0.7867347  -1.5343767\n",
            "  0.29362243  2.581584    0.07590752 -0.10008037  0.49617508 -1.0410242\n",
            "  2.236837   -1.8410532  -0.85398644  1.4837507   0.4826397  -2.4488432\n",
            " -2.4774966  -0.8530749   1.2391263   0.8774368   2.477436    1.9557596\n",
            "  3.648391  ]\n",
            "Ziggs: [ 1.8821107   0.57334673 -0.356984   -2.2396662   0.3022147  -1.870735\n",
            "  2.0309381   2.5363514  -0.36876225  1.667937   -2.7882795   0.23323181\n",
            "  0.46651193 -1.7960308   0.20116822  0.9679593  -0.27643177  0.4334932\n",
            "  1.8988049  -0.72320956 -0.95577496 -0.14448027  0.4658349  -1.263243\n",
            "  1.2458152 ]\n",
            "Lulu: [-0.36567387 -2.881075   -0.11068847  0.00616232 -1.382379    2.3852897\n",
            " -0.3950168   3.035542   -2.4030993  -1.2587816  -0.79826283 -0.7678884\n",
            "  2.3511312  -0.48795286 -0.16633075  0.8900737   0.52808166 -1.8952523\n",
            " -0.8447539   1.720177   -0.21633916 -1.6033195   2.0559123   3.4238458\n",
            "  2.1245303 ]\n",
            "Draven: [-2.1844616   2.3034017  -0.97485    -3.0575147   1.1992046   3.2798383\n",
            " -3.089479    0.87973744 -1.5926841   2.5247262   1.7156537  -0.0870114\n",
            " -2.3563561  -0.6598496  -2.8654861  -2.2248514  -0.10287507 -0.01477367\n",
            " -0.34183386 -2.3425853   0.09339186  3.0668697  -0.03434772  1.2842114\n",
            "  0.7082332 ]\n",
            "Hecarim: [-0.32024512 -0.8784726  -0.69173115 -0.45645604 -1.2878007   1.7095201\n",
            "  1.63043    -0.4039008  -1.2282581   0.3749374  -0.22223595  1.1874341\n",
            "  0.9075449  -4.457293    0.15092818  0.49935585  0.47321555 -2.2200813\n",
            " -3.2042234   1.0114248   0.08558398  0.21707569  0.30396393  0.9690768\n",
            "  1.0284228 ]\n",
            "Khazix: [-1.212592   -0.25526708  2.693865    3.4822156  -0.65742296 -1.5357132\n",
            "  0.32467344 -2.4646811  -2.644531   -2.4659631   2.5999813  -0.77247614\n",
            "  0.9189849  -1.5210267  -1.4931589   1.290926    0.40891346  0.406526\n",
            "  2.298769   -1.2669754   0.48962474  0.25950497 -1.7865055  -0.26028076\n",
            "  0.9571863 ]\n",
            "Darius: [ 1.6283395   0.06992695 -3.2653198  -3.3743348  -1.1899406   1.6038169\n",
            "  0.95747924  1.0276667   2.6587272   1.4612427  -1.6874515  -3.890406\n",
            "  1.4256064  -0.83915347 -0.5086455   1.6903559  -0.31608328 -0.07958387\n",
            " -1.0719621   0.6959079  -0.00925663 -2.732666   -1.0139619  -1.114253\n",
            " -0.72091484]\n",
            "Jayce: [-0.54139477  0.258859    0.92268443  0.4040619   0.9174397   0.6582125\n",
            "  0.3432802  -0.96019703  1.2550513  -1.3073732   1.3478884   1.7551374\n",
            " -0.7204222   0.5120277  -2.1005101   1.6913993   0.7436351   2.4867465\n",
            "  0.85670155 -0.69828135 -0.9932737  -0.406664   -1.1574006   0.80738074\n",
            " -2.745347  ]\n",
            "Lissandra: [ 0.73235166  1.6611497  -0.4857603   1.0881261   2.1892562  -0.0938072\n",
            " -0.34257236  0.95763296 -2.2280273  -0.40987596  0.796223   -1.9586139\n",
            " -1.4343888  -0.753318    3.0847006  -2.3017838   0.07345117 -2.7710695\n",
            "  0.9055343   0.6961809   0.9841785  -0.10423549 -1.8203343   0.9014118\n",
            " -2.7291937 ]\n",
            "Diana: [ 3.596034    0.59059    -0.14636435 -2.2008927  -0.06941861 -1.1522593\n",
            "  1.302618    2.4686284  -0.60826534 -1.4463246  -0.2448626   0.07287133\n",
            " -0.920269    1.3911884   1.1840612   1.7704135  -0.5698987  -2.7016037\n",
            " -1.6648769  -1.1406949  -0.6521017  -0.1233011   1.2396208  -1.3793402\n",
            " -1.2809391 ]\n",
            "Quinn: [ 1.3016646  -1.5341251   1.4663057  -1.1208653  -2.1209366   1.9981984\n",
            "  2.0662115   1.7800745   0.9256517   2.478647   -2.5957398   0.42841208\n",
            "  1.3182749   2.2050498   1.9346775  -1.0712206   6.222293    4.6330223\n",
            " -4.153886    1.9467103  -1.9346896   2.4420912   1.107322   -2.9729035\n",
            "  0.02073526]\n",
            "Syndra: [ 0.84507495 -1.364723    1.584055   -1.7358574  -1.7325357   2.079462\n",
            "  1.9086872   2.5959585  -0.00876986 -0.8675741   2.131692   -0.39887562\n",
            " -0.8730395  -3.523934   -0.8824228  -0.2984602   2.2512603   1.4701025\n",
            " -0.66593087  1.2190273   0.08967595 -0.12186195 -0.8177114  -2.201611\n",
            "  1.8967886 ]\n",
            "AurelionSol: [ 1.7737466   0.09451473 -1.648004   -3.125999    2.6818972  -1.052853\n",
            " -0.654254    0.67439735 -0.01376196  3.7860084  -0.5656111  -0.5666105\n",
            "  0.8955921   0.51653314  1.6014018   3.72876    -2.8709266   0.7692189\n",
            "  1.9600706   1.7944398  -0.59894776 -0.6908839  -0.1956662  -2.6102283\n",
            " -1.5684319 ]\n",
            "Kayn: [-0.604752    0.48412108  0.3084341  -1.0592202  -1.0052117  -0.06807861\n",
            " -1.3901742  -1.1450292  -1.7052231  -0.39550436 -0.06785013 -1.8395565\n",
            "  1.079216    0.46554115  2.4993422  -0.8019784  -0.6736292  -2.1530917\n",
            " -0.6370743  -0.7106786   2.2481275  -1.605959    0.5591346   1.7481399\n",
            "  1.1725576 ]\n",
            "Zoe: [ 1.5158032   1.7868923  -1.0781084  -0.46820706 -0.36134535 -0.54774165\n",
            "  1.5917271   3.180418   -0.24587646  0.7191047   0.74082947 -1.5309373\n",
            "  0.01638366 -0.5756178   0.5359674  -0.829206    0.03908769 -2.9168603\n",
            " -1.8441713  -4.0038767   1.5290102   1.189688   -1.4558076   1.670278\n",
            " -0.02432505]\n",
            "Zyra: [ 1.7927512   0.04272608  1.1676886   0.19762005 -1.6683288  -1.7153009\n",
            "  2.6497993   1.5856689   1.0824753  -1.5400953   0.08733287 -2.587508\n",
            "  0.9961382   0.17241545 -0.19682078  2.092936   -1.5938606   1.9370241\n",
            " -0.3418242  -1.5624758   1.9191495  -1.7253684  -1.2749201  -1.4217179\n",
            " -1.0392339 ]\n",
            "Kaisa: [-1.2829646   1.5115292   2.0390918  -0.17689909  3.2240248   1.8330177\n",
            " -1.990935    1.0498958   0.86467695 -0.4465669   1.3872843   0.6353231\n",
            " -1.3410591  -0.9501802  -4.0656557   0.44801158 -0.901544    0.95808285\n",
            "  2.615601   -1.1404724  -2.3594015   1.8592707  -1.757668    3.036337\n",
            " -2.855826  ]\n",
            "Seraphine: [ 1.1600116   0.08009011  0.31388232  0.5054509  -0.6508862   3.8311741\n",
            " -0.5189071   0.9515856  -1.1131525   0.7712394  -0.5630987   0.6373259\n",
            "  2.306172   -2.838258    0.52490187  1.4230357   2.755042    0.24048641\n",
            " -1.903098    1.5050173  -0.5826545   1.0360074  -2.5892684   2.3441367\n",
            " -2.4057636 ]\n",
            "Gnar: [-1.3222471   1.3549178  -1.1026734   3.2691548   0.9474667   1.8177929\n",
            "  0.4077431  -2.4326334  -1.5166959   1.3153032   1.7078177   2.2780855\n",
            " -1.6436584   0.42589837 -0.08697543  0.16526681  0.49489602 -0.10752436\n",
            "  0.91044337  1.0560164  -0.19441356  0.753155   -1.8464156  -0.72760046\n",
            " -1.9216305 ]\n",
            "Zac: [ 2.1594937  -0.33235863 -0.7403612   1.7204465  -1.5582962  -2.6344147\n",
            "  2.7230396  -0.30315682 -0.02587296  1.6742331  -0.08911125 -1.6494478\n",
            "  0.04555383  0.7997315   0.8218896   1.2963555  -0.15973085  0.5828058\n",
            " -1.722144    1.3423456   0.31350714 -1.9828382  -1.4491278   1.9016838\n",
            " -1.5288287 ]\n",
            "Yasuo: [-1.1664976  -1.0342275  -1.6932564  -1.6140405   0.7447855   1.2691846\n",
            "  0.42512485  3.7962956  -1.5779191  -0.5337972  -0.05821444 -1.389037\n",
            "  1.2193531   1.8180839   0.8505744  -0.49202442 -2.8538358   1.1955353\n",
            "  0.7883255   0.73421234  0.5664709   0.61701965  0.266719    0.4984044\n",
            "  3.4781709 ]\n",
            "Velkoz: [ 0.79010755 -0.7468765   0.2436859   0.358791    1.4558698  -3.2613378\n",
            "  0.5687201  -2.2411144   2.2431815   1.2762246   0.03779485  3.045126\n",
            "  3.198078   -0.8454595  -1.2460341   0.2972391   3.2239327   2.1436784\n",
            " -1.9683242   0.9916552  -3.7411652   3.3500092   2.3566957   0.95324856\n",
            "  0.0154816 ]\n",
            "Taliyah: [ 0.7060747  -0.7476596  -1.1896667  -0.6958325  -1.1552751   4.105586\n",
            "  0.29182553 -3.5155745  -0.48514774  0.9984894  -0.62471265  1.0040703\n",
            " -1.3109237  -0.01321845  1.0510453  -0.06897312  3.3795578  -1.1814318\n",
            " -1.115963    1.5586042   0.4305425   1.905771   -0.09848772 -0.83434844\n",
            " -1.3901894 ]\n",
            "Camille: [ 1.6424577  -0.8864492   0.9999305  -1.1573559   0.6690115  -1.1654731\n",
            " -0.4212393   3.5491319  -0.28436992 -0.7544077  -0.03135944 -1.3158007\n",
            "  0.5786941  -0.47472614  3.434007    0.13549519 -2.1614888  -3.9131896\n",
            " -1.2873955  -0.5311787  -0.02268201 -2.7920046   2.1316648   0.37748381\n",
            "  4.3791637 ]\n",
            "Akshan: [-0.3932752   0.6042111  -0.23696434  0.6061093   0.33155474 -3.7859015\n",
            "  2.491684    1.4883235  -2.4496336   3.4839132  -1.30736    -1.5257323\n",
            "  1.7424611   5.076185    2.6538002   4.349251   -0.21332692 -1.9131387\n",
            " -0.5767855   1.3508157   1.5415468   1.3044788   2.9733555   3.5867317\n",
            "  1.1007898 ]\n",
            "Belveth: [ 0.7470591  -2.7537317  -3.4607542  -1.3769206  -1.6840206  -2.2737646\n",
            "  0.99955434 -0.8453057  -1.1642001   1.7652217  -2.8032546   0.32472387\n",
            "  0.0302083   1.8475494  -3.0066817   1.0117221   1.1484787  -1.4179602\n",
            " -1.9959834   2.2418935   0.89083415  0.75126165  1.8310525  -1.8201835\n",
            "  4.836774  ]\n",
            "Braum: [-0.21254115 -3.09258    -1.3395606   0.34777385 -1.8604542   0.559974\n",
            "  0.47170502 -1.3737366  -0.7502522  -2.8522909  -2.4571493  -0.19079846\n",
            "  0.3172593   1.4568814  -0.6751167  -0.27005988  0.80819064 -0.42254657\n",
            "  1.3207258   1.9894445   1.8001323   1.3797883   0.9913548   0.6246044\n",
            "  0.43096322]\n",
            "Jhin: [-0.65925425 -0.42892852  0.17050354  0.339598    2.0202868  -1.713512\n",
            " -0.3592054  -1.57713     0.92753005  0.09303106  0.6818518   1.4631813\n",
            " -0.5594184   0.44497368 -0.18943357  2.8530068   1.3649676   1.1464593\n",
            "  3.6271598  -1.416265    0.5252433   2.5331008   0.0118905   0.30491704\n",
            "  0.6305144 ]\n",
            "Kindred: [-2.4169934  -3.5685074  -0.8973114   0.29038018 -1.3135061  -0.86442953\n",
            "  0.75660044  3.218549    0.27751973 -3.2011468   0.5570452  -2.2963018\n",
            "  2.1254313  -5.074543    2.814469   -2.93944    -0.48957682  0.17118396\n",
            "  0.41845438  2.4022062  -1.116313   -1.4401606   1.3128697   1.8746965\n",
            "  1.3788052 ]\n",
            "Zeri: [-3.7184017   2.5262158   0.9637938   0.38535264  3.6606987  -2.9376876\n",
            " -0.7490248   0.6392341   1.8116063  -0.1620742   3.0916545  -1.2252469\n",
            " -3.0796826   1.0250095  -0.58269995 -3.241918    2.306776    2.1758425\n",
            "  2.2619998  -0.4115539   0.03393412  0.88524604 -0.90645087  3.0049272\n",
            "  0.1581265 ]\n",
            "Jinx: [-0.64670926  1.3222457   0.816909    1.7484314   2.3445745   0.21622929\n",
            "  1.4922765   0.86104923 -1.4880437   1.8013113   0.16518418  1.7322053\n",
            " -0.9921736   0.4205682  -0.04081247  1.2924724  -0.3057778  -0.09901559\n",
            "  2.1326628  -0.72377664 -0.68466574 -1.905409    0.70946914 -0.32545766\n",
            " -1.0917305 ]\n",
            "TahmKench: [ 3.3626568   1.3735749  -0.6877212   0.42506152 -1.3952159  -3.282132\n",
            "  0.8793446  -0.794706   -0.17098993 -0.33234897  0.41682288  1.1047403\n",
            "  2.268325   -1.1624845  -0.13622561  3.0189798  -0.3945833  -0.44255155\n",
            " -0.105238    1.2755079  -0.14216283 -1.9866028  -1.3297111  -1.3882849\n",
            " -0.95014703]\n",
            "Briar: [ 3.1657233  -1.8425744   0.77765363  0.5498391  -0.56067437 -0.26787096\n",
            "  0.68561506 -0.02327673 -1.3093164  -0.60135335  1.3629973   0.83712685\n",
            " -3.1980138   2.071325    3.24643     0.7178095   1.698788   -1.8798497\n",
            " -2.453742    2.3824987   1.018113    0.91160756 -4.102802   -0.56058717\n",
            " -2.549767  ]\n",
            "Viego: [ 1.5184915   0.33385864  0.05918578  1.7801194   0.7303968   1.0150584\n",
            "  0.7488612  -0.5102527  -3.7042444  -0.49461052 -0.884499   -2.5041761\n",
            "  0.10845669 -0.9441667   0.71624714 -1.9589494  -1.3616394  -0.1439911\n",
            "  3.5618498   0.68260735  0.90024817 -0.17915687  0.11881279 -1.1365913\n",
            " -1.1369878 ]\n",
            "Senna: [ 0.24359305 -2.4483035   0.02526701  3.034076   -0.47541553  3.7783926\n",
            " -0.23365867 -2.2902257   1.8910652  -0.2980401   1.3660654   0.65161467\n",
            " -1.012323   -2.0804741  -1.7394363  -1.7039891   2.2663996  -1.4918256\n",
            " -0.65789866 -2.1241941   0.4726015   0.9063605  -0.04798975 -2.7230654\n",
            " -1.4745476 ]\n",
            "Lucian: [-2.4026475   3.5163493  -0.3306467   0.6451907   2.720983    0.06646831\n",
            " -1.0308065  -0.53887534 -0.5548154   0.47688916  1.2338972  -0.02195989\n",
            " -1.1771886   0.13676652 -2.3518374  -1.2653638  -1.2719592  -0.69479525\n",
            "  1.6643738  -0.6656247  -0.23132285  0.75464517 -2.781226    3.3462906\n",
            "  0.25431424]\n",
            "Zed: [-0.35847953  0.11799577  0.4181028   3.9718883  -0.743217   -1.398101\n",
            "  1.1707406   1.8791012  -0.77918434  2.082781    0.34708637  1.9907198\n",
            "  1.6094974  -3.1324346   0.34404415 -2.253941    0.92327285  0.5044016\n",
            "  0.91168493 -0.5922497  -4.7444572   1.9236453  -1.9420692  -0.95100355\n",
            " -3.2250876 ]\n",
            "Kled: [-1.4093194  -2.8548477  -0.49561515 -4.213377   -0.43158808 -2.2958682\n",
            "  1.3537534  -0.8055836   2.974396   -1.8015989  -0.30816877  0.2393465\n",
            "  1.3007575   0.09688877 -4.0174003  -0.63737404  1.2974763   3.9271958\n",
            " -0.23388824  1.4776325   1.5773139   3.1787825   3.2880228   1.0945148\n",
            "  2.9552414 ]\n",
            "Ekko: [ 0.85039324 -1.4618417  -1.3118503  -0.1186476  -1.2432029  -0.44954598\n",
            "  1.2032657  -3.9204154  -0.9823805   0.04029293 -0.39314362  0.87384903\n",
            " -0.7680874  -0.86640286 -0.43469772  0.04362049  1.2331605  -1.222463\n",
            "  0.253382    2.839499    1.5557189   1.04985     1.6324931   0.24666362\n",
            "  1.3198963 ]\n",
            "Qiyana: [ 1.3487538  -0.37025604 -0.63860863 -2.0723565   2.2542086  -0.05418871\n",
            " -0.18586175  3.8569763  -1.7482489   1.9150269   1.3124516  -0.8657435\n",
            "  2.8903553  -0.45714027  1.4927195   0.48273322  2.7486365  -0.07569434\n",
            "  0.7159204   0.8167669  -2.2318041   0.76972765  2.3322673   0.9976369\n",
            "  1.7783253 ]\n",
            "Vi: [ 0.24896045  0.22956942 -1.2810024   0.04241264  1.096556    1.5672784\n",
            " -1.5843827  -0.7446131  -0.01540319 -3.0227256  -0.84424174 -0.2904347\n",
            "  0.9951066  -1.0800096  -0.03398546 -1.2270788  -3.4912534  -1.1934159\n",
            " -0.13677607 -0.2764689  -0.7073235  -1.7603867   0.00842234 -0.9000081\n",
            " -2.24455   ]\n",
            "Aatrox: [-1.2111335  -0.01154313  0.10935083  0.17504527  0.509528    4.0138006\n",
            "  0.8141777   0.7195218   0.09458121 -1.1006532  -0.40092322 -1.5660185\n",
            " -0.86669135 -2.0916014   1.1418886  -3.1639395   0.10931795 -1.9312519\n",
            " -0.85487384 -0.51267767 -1.9504217  -0.39559045  0.8081421   1.7798346\n",
            " -1.5625302 ]\n",
            "Nami: [ 1.529022    0.19524075  0.14971818  0.48465028 -1.2556475  -1.9695764\n",
            "  2.0117264   0.5808454   0.73839843 -0.84474754 -1.7152407  -0.51357305\n",
            "  1.1553653   0.36190414  1.9379348  -0.74903756  0.2464054   0.88243484\n",
            " -2.3248625   1.3175825   2.2348788   1.0968695   1.8318806  -0.9767998\n",
            "  0.86838657]\n",
            "Azir: [-1.4680308   3.001185    1.6295847   3.0839484   3.1801236   0.8913816\n",
            " -3.4860487   1.0521189   1.4928727  -0.00980139  1.3019581  -1.1122379\n",
            "  2.545713    1.6426065  -0.9033171   2.750473   -3.9221058   1.7548971\n",
            "  2.3623426  -1.2035041  -1.3102474   0.7184827  -0.5210695   1.0915205\n",
            " -3.4548473 ]\n",
            "Yuumi: [-0.39597103 -0.6501627  -3.7864351  -0.5370876   0.42694056 -0.14652279\n",
            "  0.7781378   2.1859312  -1.8279611   1.2987603   0.10311385 -1.055883\n",
            "  1.0948831   0.710006   -1.4349588   1.9083189  -3.5902686  -3.164462\n",
            "  1.5663328   0.6620484  -1.5502809  -2.613666    0.1470325   0.85876346\n",
            "  2.6337337 ]\n",
            "Samira: [-2.764392    3.7253263  -1.414678   -1.0351197   0.70263    -4.116859\n",
            " -1.1317028  -0.7354214  -0.9759289   3.6100912   0.05306438  2.5640836\n",
            " -0.95968944 -0.23679693 -0.9325675   1.1599323  -0.01179798  0.605297\n",
            "  0.91434556 -0.71263283 -1.7744255   1.2647197  -2.348014   -0.4066189\n",
            "  2.2569282 ]\n",
            "Thresh: [ 2.534618   -2.3549025  -1.3059386   1.0164201  -2.0006862   1.7521636\n",
            "  1.7016586   1.4433825  -0.44374537 -2.0006547   0.9975785  -0.6251153\n",
            " -0.46685266 -2.4191349   1.3891901   1.1772841   0.42197296 -2.6228423\n",
            " -1.998349   -1.3029761   0.5322878  -2.7978911   2.0780518   0.6624808\n",
            "  3.5025718 ]\n",
            "Illaoi: [-0.73995066  2.7835746   4.1276445  -2.7300253   5.2131476   2.243002\n",
            " -1.083732   -2.1519375   3.857757   -1.8928127   0.30034038  3.5380955\n",
            "  0.8111417   2.8726928  -2.3628578   5.550706   -3.5937402   1.2673393\n",
            "  0.7112372  -1.1606754   1.676843    2.413683   -1.0285426   1.025284\n",
            " -1.1993647 ]\n",
            "RekSai: [ 0.01993258 -2.2227876  -3.643671   -6.1849937  -3.1932487   0.2799747\n",
            "  0.7777922  -2.8015766   0.51128113  1.1997164   1.3479158  -1.4083867\n",
            "  1.0475171   2.9114554  -2.9576776  -1.2339119   1.4424163   1.3725761\n",
            " -3.9744     -1.7075067  -2.876901    2.7768998  -1.1875886  -0.05305354\n",
            "  1.4757462 ]\n",
            "Ivern: [-2.2478518   0.66458493  2.0614552  -2.6254375  -1.6920321  -3.0812626\n",
            "  0.21857776 -0.26996523  1.4610392   0.61778736  1.8059177   1.524775\n",
            "  0.01112872  0.88230747 -2.896554   -0.4287422   3.4530659   1.9429289\n",
            " -1.2704614  -1.1965151   0.4213882   5.5503373  -0.03854322  0.48703355\n",
            "  0.5253413 ]\n",
            "Kalista: [-1.1755321   1.8513644   3.6564531  -0.98172295  1.2799226   2.0587993\n",
            "  0.43429655 -4.0465736   1.7738651  -0.48965052 -0.6932967   0.3637214\n",
            " -2.1336966  -1.8417219  -2.9729428  -1.5283529  -2.7414823   2.8399763\n",
            "  1.3474889  -4.2466555  -2.2564998   3.0721793  -2.7504532   1.0865259\n",
            " -3.4444559 ]\n",
            "Bard: [ 1.467059   -0.96985555  1.1400056   2.887201   -0.7270171  -0.81584513\n",
            "  0.70829415  0.69297796  2.1633615  -3.0063791  -1.6940359  -0.1466927\n",
            "  2.698346    0.12748589  0.53794503 -0.25491065  0.06756292  1.1548976\n",
            " -0.9569976  -0.4195278  -0.3285843  -1.4771771   2.576496   -0.07861721\n",
            " -1.729837  ]\n",
            "Rakan: [-0.6545919   1.6095694   0.7315669   0.5612535   0.7805851  -2.8365304\n",
            " -1.9400299  -2.3162425  -0.55016893 -2.4911716   0.842196    3.1214051\n",
            " -2.8044689   2.2226424   0.5381607   0.7854333   1.1396599  -0.7371487\n",
            "  2.3905163  -0.41353372  2.6484728   1.5566357  -0.93176186  2.7081423\n",
            "  0.91987854]\n",
            "Xayah: [-0.628339    2.1395922   1.8711702   0.9837361   1.4808736   0.19923529\n",
            " -1.1078604  -2.2389565   2.0450013   0.21131867  1.8544053  -0.8933276\n",
            " -0.91128445  2.0319965   0.8360837   1.9557288   2.6694036   1.8326524\n",
            "  4.080263    0.2849446   1.5016819   1.8047951  -1.9924389  -0.7115057\n",
            "  1.8469822 ]\n",
            "Ornn: [-0.04853054 -0.2649053  -0.15145367  3.616544   -1.9294224   0.12410688\n",
            " -0.07403433  2.141777   -0.2611065  -0.23179068 -1.1009207   0.63202435\n",
            "  1.7097638  -0.59217083 -1.4259099   0.55820084 -0.2402687  -1.2010148\n",
            " -2.599076    1.2115773  -0.24194138 -1.4469703   1.2541003   3.1614115\n",
            "  0.8284789 ]\n",
            "Sylas: [-2.128969    1.2167513  -0.7692463   1.8193052  -0.71340567  3.8914855\n",
            " -0.92450833 -2.6286905  -0.38044393 -1.1233461   0.0243223   1.9737968\n",
            " -1.2388017  -1.0285745   0.7804297  -1.6199496   0.17302981  2.6006587\n",
            "  1.4200425   0.41891095 -3.3941915   0.21492119 -1.1277969  -2.2562754\n",
            "  0.544223  ]\n",
            "Neeko: [-1.952199   -1.8163757   0.972248   -0.19308041  1.6579808   2.0326455\n",
            "  0.03563972 -0.67780036 -0.38658443 -0.36591554  2.3700867   1.7893878\n",
            " -1.1723377   3.3981194   1.7514409  -1.5373199   1.0505532   2.9435394\n",
            "  0.72163934 -2.7462637  -0.875888    3.5841625  -2.2257802  -1.6799965\n",
            " -1.5905035 ]\n",
            "Aphelios: [-1.6925654   1.6027621   3.1521456  -2.8948722   3.445903   -1.453252\n",
            " -0.6337839  -2.2134027   1.0411842  -0.01035592 -1.5057538   1.891264\n",
            " -1.5389326   2.3259578  -2.606141   -1.933691    0.32943168  0.10674523\n",
            "  0.45773873 -2.8276985   1.2414274   5.3607144  -2.361606   -0.82165825\n",
            " -1.7709072 ]\n",
            "Rell: [-2.0230222  -0.20779967  2.34271    -0.8993542  -2.5469632   0.3325308\n",
            "  0.694248   -2.9136586   0.36852404 -3.536455   -0.2596486   1.9252167\n",
            " -1.355182    0.97719896 -3.5568066  -0.93845165 -0.84198976  0.64877915\n",
            " -0.4497558  -1.2042755   1.3281302   1.3462026  -0.80397964  2.197717\n",
            "  1.0816618 ]\n",
            "Pyke: [ 0.00775893  2.2494116  -0.9376134   1.399076   -1.5917263   2.8839588\n",
            "  1.8035158   0.08590633  3.52245     1.5086641  -1.6785158  -1.1774943\n",
            "  0.63274133 -1.490465    1.7672931   0.2115321   3.1360023   3.2189445\n",
            "  3.7838054   1.651511   -0.6603992  -0.76195097  0.23196833 -3.1295648\n",
            " -0.8614935 ]\n",
            "Vex: [-1.4552324  -3.482556    0.7862159  -0.89208835 -0.84212977  0.62834513\n",
            " -2.3376338   0.32806683  0.77722186 -0.61455363  0.59650135 -0.60977423\n",
            "  0.302031   -0.11765047 -0.9979408  -1.2235706  -0.89909035 -1.4226143\n",
            " -2.155312    1.4781021  -2.3573117  -1.6092238   0.6054634   1.1827633\n",
            " -1.6871974 ]\n",
            "Yone: [ 0.821026   -1.3009285   2.5510273   2.2917218   2.2621214   1.1521387\n",
            " -2.7545478  -3.0014076   1.0523485  -1.1968569  -0.8916718  -0.362114\n",
            " -2.5416555   0.09161428 -3.7981997   0.04760766 -2.101147    0.25563952\n",
            "  1.0196565   0.25180435 -1.346125    1.3324958  -1.2228087  -2.5332413\n",
            " -2.32444   ]\n",
            "Ambessa: [-1.6004372   1.3008794   1.0105575   0.4203988   1.5372013   0.31322187\n",
            "  0.2828243  -0.574207   -3.6593256  -0.16606149  2.7704556   0.35242417\n",
            " -2.0493891   2.2710705   1.4418796  -1.077243    0.580366   -3.649331\n",
            "  1.1415417  -2.251486    3.044336   -2.3816671  -3.243302    2.072244\n",
            " -1.51186   ]\n",
            "Mel: [-0.71526355  3.7930777   0.3328894  -1.7921101   2.7288105  -0.74730337\n",
            " -0.6493847  -0.6664622   1.4774255   1.9535351   1.1002059  -0.8398936\n",
            " -0.69879764  0.48639196 -0.8540653   2.7762067  -4.5901604   3.213675\n",
            "  0.6340714   0.5723002   0.33570817 -1.1174585  -0.9559342   0.11467424\n",
            " -1.2066177 ]\n",
            "Yunara: [ 0.06660253  0.0838549   0.10934329 -0.13229959 -0.09042634  0.04561316\n",
            " -0.05580309  0.03567817 -0.13600768 -0.1584368   0.06797375  0.03756031\n",
            " -0.01316795 -0.08629047  0.052764   -0.02997627  0.15896317 -0.13966869\n",
            "  0.14000408  0.02853407  0.0404275  -0.04211494  0.0953487   0.13145758\n",
            " -0.1746767 ]\n",
            "Sett: [ 0.7750958  -2.1146646  -0.5460714   5.2786293  -1.4236256  -1.5890762\n",
            "  2.1364331   1.3975109  -2.063892    0.5514849   2.7565384   0.6343813\n",
            "  3.7992647  -3.9423966   4.0473547   1.6255152   0.10156018 -1.7968587\n",
            " -0.98408043 -0.16736875  1.6300431  -0.24138202  3.6284592  -1.5974956\n",
            " -2.869274  ]\n",
            "Lillia: [ 0.9542992  -2.1570044  -2.397936   -1.4958087  -1.2502763   1.2825077\n",
            "  0.7584082  -1.2027525   0.7461822   0.33643028 -0.6186382  -1.9753697\n",
            "  1.3843066   1.5382842  -1.9614366  -0.50717765 -1.3412882  -1.4401921\n",
            " -2.2192988  -0.48541343  3.1378896   1.5776854   1.741607   -0.11883327\n",
            "  3.5406992 ]\n",
            "Gwen: [-1.8817832  -2.3356254  -0.5482526   0.43314937 -1.1496382   0.11232267\n",
            " -1.8459665  -0.96336114  1.7709318  -0.9529673   2.3309608   0.47261935\n",
            " -0.93397874  0.68127865 -1.0267881  -1.488518    0.21707752 -0.44836646\n",
            " -2.618674   -0.42949244  1.421552    1.0060686  -0.16460425  2.958858\n",
            "  1.1054262 ]\n",
            "Renata: [-0.44131377 -0.46920332 -1.6453522  -0.6948509   0.7696053  -0.50260615\n",
            " -1.9092201  -0.3926839  -1.2833923   0.42642438 -2.887297    1.4141587\n",
            "  0.2025888   2.3918293   1.9769759   1.181623   -2.2929726  -0.90372145\n",
            "  1.3948425   1.1844609   2.021991   -2.6343758  -0.07772032 -1.4766124\n",
            "  0.83403176]\n",
            "Aurora: [-0.2897483  -2.0703096   3.2207575  -0.01357669  2.0930905   0.24751157\n",
            " -0.40938234 -0.82736224  1.4868542  -2.2164881   0.8755475   2.9521866\n",
            "  1.1498022   2.2448277  -1.4932077  -1.238247    1.0725508  -0.5121224\n",
            " -0.3646478  -2.7995367  -1.2832856   1.7175338  -1.3828685   3.2271168\n",
            " -0.5491413 ]\n",
            "Nilah: [-0.7476984   0.7511434  -0.27833775  0.30156356 -0.6169267  -1.6849751\n",
            "  0.81455606 -1.4835572   1.0781378  -1.6959232   2.3328469  -3.221148\n",
            "  1.7787963  -4.934134   -3.7351434   1.911031   -1.1083403   1.7252061\n",
            " -0.8078066   0.10448181 -0.80803216  0.13184917  0.41568017  1.0691488\n",
            " -0.9690911 ]\n",
            "KSante: [-2.8548322  -0.4367343  -1.0265224  -1.3249559   1.326465   -0.840421\n",
            "  1.6311312   0.85375243 -2.3965323  -3.0494146  -0.81564045 -0.37424415\n",
            " -0.35533842  1.23961    -0.26756442 -0.39352527 -1.8735108   1.995479\n",
            "  1.5469177  -0.07994625  2.3353658   1.1403674  -0.735161    1.0633622\n",
            "  1.0251129 ]\n",
            "Smolder: [-0.5110261   2.68651     0.70636135 -3.0645533   3.5155602  -1.4747335\n",
            " -2.6072628  -2.034721    1.5894245   2.2760413   0.8756183   1.5715144\n",
            " -1.7467105   3.3135138  -2.539368    0.5843654  -1.4545274   0.15316962\n",
            "  2.1429255  -1.1751833  -1.9536535   1.9345036  -0.78786737 -1.2065285\n",
            " -1.422668  ]\n",
            "Milio: [ 0.4945981  -1.3514178  -0.87688035 -1.2134062  -1.4068477   2.2045279\n",
            "  1.0795456   0.7807683   1.5413071  -1.0418992  -1.6389143  -0.93634117\n",
            " -0.51732534 -1.2433742   0.30348194 -2.2683544   2.1972907  -1.6888748\n",
            " -0.33353063  1.0974752   0.5724254   2.557389    1.6910369  -1.8522863\n",
            "  0.3889439 ]\n",
            "Hwei: [ 0.46581882  2.8715193  -0.78306943 -1.4639845  -1.3977357  -0.97925293\n",
            "  2.023167   -1.9751614  -1.629136    3.4833105   1.0596098   0.00837778\n",
            " -1.1325313   1.2941152  -0.4366413   2.219399    0.46881902  1.9012706\n",
            "  2.1189167  -0.07216576  1.1099865  -1.3838456  -1.3446008  -2.3323836\n",
            " -3.0736217 ]\n",
            "Naafiri: [ 0.55178946 -0.5416085  -0.50778794 -0.7277593  -0.83924127  1.1529804\n",
            " -1.1083338  -0.5641062  -1.7613393   1.2621299  -1.7861099  -0.56900406\n",
            "  0.56542706  1.1920502   3.0185244  -2.6798384   1.8857464  -1.4226999\n",
            " -2.9824586  -2.0857832   0.76471895 -0.41347715  1.1940355  -1.3915344\n",
            "  1.1556044 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR36UlvGO0Rd",
        "outputId": "7596283b-acce-483c-f1e6-45d632e947f4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChampModel(\n",
            "  (embedding): Embedding(171, 25)\n",
            "  (attn1): MultiheadAttention(\n",
            "    (out_proj): NonDynamicallyQuantizableLinear(in_features=25, out_features=25, bias=True)\n",
            "  )\n",
            "  (norm1): LayerNorm((25,), eps=1e-05, elementwise_affine=True)\n",
            "  (mlp1): Sequential(\n",
            "    (0): Linear(in_features=25, out_features=25, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=25, out_features=25, bias=True)\n",
            "  )\n",
            "  (norm2): LayerNorm((25,), eps=1e-05, elementwise_affine=True)\n",
            "  (attn2): MultiheadAttention(\n",
            "    (out_proj): NonDynamicallyQuantizableLinear(in_features=25, out_features=25, bias=True)\n",
            "  )\n",
            "  (norm3): LayerNorm((25,), eps=1e-05, elementwise_affine=True)\n",
            "  (mlp2): Sequential(\n",
            "    (0): Linear(in_features=25, out_features=25, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=25, out_features=25, bias=True)\n",
            "  )\n",
            "  (norm4): LayerNorm((25,), eps=1e-05, elementwise_affine=True)\n",
            "  (output): Linear(in_features=25, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as F\n",
        "import torch\n",
        "\n",
        "def evaluate(model, data, batch_size=32):\n",
        "    model.eval()\n",
        "    all_losses = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(data), batch_size):\n",
        "            batch = data[i:i + batch_size]\n",
        "\n",
        "            batch_blue = [[champ['championName'] for champ in item['blueTeam']] for item in batch]\n",
        "            batch_red = [[champ['championName'] for champ in item['redTeam']] for item in batch]\n",
        "            labels = [1.0 if item['winningTeam'] == 'blue' else 0.0 for item in batch]\n",
        "\n",
        "            blue_scores = model(batch_blue)\n",
        "            red_scores = model(batch_red)\n",
        "            preds = torch.sigmoid(blue_scores - red_scores)\n",
        "\n",
        "            target = torch.tensor(labels, device=preds.device,dtype=torch.float32)\n",
        "            loss = F.smooth_l1_loss(preds, target, beta=0)\n",
        "            all_losses.append(loss.item())\n",
        "\n",
        "            # Accuracy\n",
        "            pred_labels = (preds > 0.5).float()\n",
        "            correct += (pred_labels == target).sum().item()\n",
        "            total += len(batch)\n",
        "\n",
        "\n",
        "    avg_loss = sum(all_losses) / len(all_losses)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return avg_loss, accuracy\n"
      ],
      "metadata": {
        "id": "3WW5HLjYOLVE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def load_jsonl_range(path, start, end):\n",
        "    with open(path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    return [json.loads(line) for line in lines[start:end]]\n",
        "\n",
        "eval_data = load_jsonl_range(jsonl, 3_570_000, None)\n",
        "\n"
      ],
      "metadata": {
        "id": "mVU7lCrEfPmw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = evaluate(model, eval_data)\n",
        "print(f\"Evaluation Loss: {loss:.4f}\")\n",
        "print(f\"Accuracy: {acc*100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAsdQ336OOoi",
        "outputId": "d1126998-9119-4494-d590-1f8fc87ddcd8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Loss: 0.4986\n",
            "Accuracy: 50.19%\n"
          ]
        }
      ]
    }
  ]
}